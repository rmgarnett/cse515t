<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>CSE 5015: Bayesian Methods in Machine Learning &ndash; Fall 2025</title>
    <meta name="description" content="Course webpage for CSE 5015: Bayesian Methods in Machine Learning, Fall Semester 2025" />
    <meta name="author" content="Roman Garnett" />
    <link href="https://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Droid+Sans+Mono" rel="stylesheet">
    <link rel="stylesheet" href="style.css" />
    <!--[if lt IE 9]>
	<script>
	  document.createElement("header" );
	  document.createElement("footer" );
	  document.createElement("section");
	  document.createElement("aside"  );
	  document.createElement("nav"    );
	  document.createElement("article");
	  document.createElement("hgroup" );
	  document.createElement("time"   );
	</script>
	<noscript>
	  <strong>Warning!</strong>
	  Because your browser does not support HTML5, some elements are simulated using JScript.
	  Unfortunately your browser has disabled scripting. Please enable it in order to display this page.
	</noscript>
	<![endif]-->
  </head>
  <body>
    <header>
      <h1>CSE 5015: Bayesian Methods in Machine Learning &ndash; Fall 2025</h1>
      <p>Instructor: Professor Roman Garnett<br />
	TA: Ryan Zhang<br />
	Time/Location: Monday/Wednesday 10&ndash;11:20am, Seigle 301<br />
	Office hours (Garnett): by appointment, always on Slack<br />
	<a href="files/syllabus.pdf">syllabus</a><br />
	<a href="https://join.slack.com/t/cse515t/shared_invite/zt-3bx4m5x7l-cAtGJ6pDwRpt85uSCUTblQ">Slack invite link</a></br />
    </header>
    <hr />
    <section>
      <h2>Description</h2>
      <p>This course will cover modern machine learning techniques from a Bayesian probabilistic perspective. Bayesian probability allows us to model and reason about all types of uncertainty. The result is a powerful, consistent framework for approaching many problems that arise in machine learning, including parameter estimation, model comparison, and decision making. We will begin with a high-level introduction to Bayesian inference, then proceed to cover more-advanced topics.</p>
    </section>
<!--
    <section>
      <h2>Project</h2>
      <p>You can find detailed information about the project <a href="files/project.pdf">here.</a></p>
    </section>
    <section>
      <h2>Assignments</h2>
      <p>Please post questions to Slack!
      <p><a href="files/assignments/assignment_1.pdf">Assignment 1</a>, due <strong>18 September 2024.</strong><br /></p>
      <p><a href="files/assignments/assignment_2.pdf">Assignment 2</a>, due <strong>2 October 2024.</strong><br /></p>
      <p><a href="files/assignments/assignment_3.pdf">Assignment 3</a>, due <strong>13 November 2024.</strong><br /></p>
    </section>
-->
    <section>
      <h2>Lectures</h2>
      <section>
	<h3 class="lecture">Lecture 1: Introduction to the Bayesian Method</h3>
	<span class="time">Monday, 25 August 2025</span><br />
	<a href="files/lecture_notes/1.pdf">lecture notes</a>
	<p>Additional Resources:</p>
	<ul>
	  <li>Book: Bishop PRML: Section 1.2 (Probability theory)</li>
	  <li>Book: Barber BRML: Chapter 1 (Probabilistic reasoning)</li>
	  <li>Book: Garnett BayesOpt: Section 1.2 (The Bayesian Approach)</li>
	  <li>Book: <a href="https://dataorigami.net/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">Bayesian Method for Hackers</a> (Cam Davidson Pilon) Great high-level overview from an atypical perspective!</li>
	  <li>Video: <a href="https://www.youtube.com/watch?v=pid0lUH467o">Introduction to Machine Learning</a> (Nando de Freitas)</li>
	  <li>Video: <a href="https://www.youtube.com/watch?v=mgBrXnjF8R4">Bayesian Inference I</a> (Zoubin Ghahramani) (the first 30 minutes or so)</li>
	</ul>
      </section>
    <section>
      	<h3 class="lecture">Lecture 2: Bayesian Inference I (coin flipping)</h3>
      	<span class="time">Wednesday, 27 August 2025</span><br />
      	<a href="files/lecture_notes/2.pdf">lecture notes</a>
      	<p>Additional Resources:</p>
      	<ul>
      	  <li>Book: Bishop PRML: Section 2.1 (Binary variables)</li>
      	  <li>Website: Wikipedia has an article on <a href="http://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair">checking whether a coin is fair.</a></li>
      	  <li>Website: Marcus Brinkmann (lambdafu) has put together a <a href="http://nbviewer.ipython.org/github/lambdafu/notebook/blob/master/math/Bayesian%20Coin%20Flip.ipynb">Python notebook on Bayesian coin flipping.</a></li>
      	</ul>
      </section>
<!--
      <section>
      	<h3 class="lecture">Lecture 3: The Gaussian Distribution</h3>
      	<span class="time">Wednesday, 11 September 2024</span><br />
      	lecture notes &ndash; see <a href="https://bayesoptbook.com/">Garnett BayesOpt</a>: Appendix A
      	<p>Additional Resources:</p>
      	<ul>
      	  <li>Book: Bishop PRML: Section 2.3 (The Gaussian Distribution).  This is a truly excellent and in-depth discussion!</li>
      	  <li>Book: Barber BRML: Section 8.4 (Multivariate Gaussian).</li>
      	  <li>Book/reference: Rasmussen and Williams GPML: Section A.2 (Gaussian Identities), <a href="http://www.gaussianprocess.org/gpml/chapters/RWA.pdf">available here</a>. This is a good cheat sheet!</li>
      	  <li>Website: The Wikipedia articles on <a href="http://en.wikipedia.org/wiki/Normal_distribution">the normal distribution</a> and <a href="http://en.wikipedia.org/wiki/Multivariate_normal_distribution">the multivariate normal distribution</a> are quite complete.</li>
      	  <li>Video: YouTube user mathematicalmonk has a <a href="https://www.youtube.com/watch?v=TC0ZAX3DA88">lecture on the multivariate normal</a> available as well.</li>
      	  <li>Video: Alexander Ihler also has a <a href="https://www.youtube.com/watch?v=eho8xH3E6mE">lecture on the multivariate normal</a>, including information on how to sample from the distribution.</li>
      	</ul>
      </section>
-->
      <section>
	<h3 class="lecture">Lecture 3: Bayesian Inference II (hypothesis testing and summarizing distributions)</h3>
	<span class="time">Monday, 15 September 2025</span><br />
	<a href="files/lecture_notes/3.pdf">lecture notes</a>
	<p>Additional Resources:</p>
	<ul>
      	  <li>Article: "The Fallacy of Placing Confidence in Confidence Intervals" <a href="http://andrewgelman.com/wp-content/uploads/2014/09/fundamentalError.pdf">available here</a> or <a href="https://link.springer.com/article/10.3758/s13423-015-0947-8">here</a></li>
	</ul>
      </section>
      <section>
      	<h3 class="lecture">Lecture 4: Bayesian Inference III (decision theory)</h3>
      	<span class="time">Wednesday, 17 September 2025</span><br />
      	<a href="files/lecture_notes/4.pdf">lecture notes</a>
      	<p>Additional Resources:</p>
      	<ul>
      	  <li>Book: Bishop PRML: Section 1.5 (Decision theory)</li>
      	  <li>Book: Berger Chapter 1 (Basic concepts), Section 4.4 (Bayesian decision theory)</li>
      	  <li>Book: Robert Section 4.2 (Bayesian decision theory)</li>
      	  <li>Videos: YouTube user mathematicalmonk has a <a href="https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA">great series of machine-learning lectures</a> available. Chapter 11 concerns decision theroy.</li>
      	</ul>
      </section>
<!--
      <section>
      	<h3 class="lecture">Lecture 6: Bayesian Linear Regression</h3>
      	<span class="time">Monday, 22 September 2024</span><br />
      	<a href="files/lecture_notes/6.pdf">lecture notes</a>
      	<p>Additional Resources:</p>
      	<ul>
      	  <li>Book: Bishop PRML: Section 3.3 (Bayesian Linear Regression).</li>
      	  <li>Book: Barber BRML: Section 18.1 (Regression with Additive Gaussian Noise).</li>
      	  <li>Book: Rasmussen and Williams GPML: Section 2.1 (Weight-space View), <a href="http://www.gaussianprocess.org/gpml/chapters/RW2.pdf">available here</a>.</li>
      	  <li>Video: YouTube user mathematicalmonk has an entire section devoted to Bayesian linear regression.  See ML 10.1&ndash;7 <a href="https://www.youtube.com/watch?v=dtkGq9tdYcI">here</a>.</li>
      	  <li>Videos: Nando de Freitas has a series of lectures on Bayesian linear regression.  Part one is <a href="https://www.youtube.com/watch?v=Fae0j1WN1zA">here</a>, and part two is <a href="https://www.youtube.com/watch?v=2KXoC6Dxhxs">here</a>.</li>
      	</ul>
      </section>
      <section>
      	<h3 class="lecture">Lecture 7: Bayesian Model Selection</h3>
      	<span class="time">Wednesday, 25 September 2024</span><br />
      	<a href="files/lecture_notes/7.pdf">lecture notes</a>
      	<p>Additional Resources:</p>
      	<ul>
      	  <li>Book: Bishop PRML: Section 3.4 (Bayesian Model Comparison).</li>
      	  <li>Book: Barber BRML: Chapter 12 (Bayesian Model Selection).</li>
      	  <li>Book: MacKay ITILA: Chapter 28 (Occam's Razor and Model Comparison).</li>
	  <li>Book: Garnett BayesOpt: Chapter 4 (Model Assessment, Selection, and Averaging).</li>
      	  <li>Video: YouTube user mathematicalmonk has a <a href="https://www.youtube.com/watch?v=1CFof6hU3cI">lecture about Bayesian model selection</a> (some nearby videos are related as well).</li>
      	</ul>
      </section>
      <section>
      	<h3 class="lecture">Lecture 8: The Kernel Trick</h3>
      	<span class="time">Monday, 30 September 2024</span><br />
      	<a href="files/lecture_notes/8.pdf">lecture notes</a>
      	<p>Additional Resources:</p>
      	<ul>
      	  <li>Book: Rasmussen and Williams GPML: Chapter 2 through 2.1 (Weight-space View), <a href="http://www.gaussianprocess.org/gpml/chapters/RW2.pdf">available here</a>.</li>
      	</ul>
      </section>
      <section>
	<h3 class="lecture">Lecture 9: Bayesian Logistic Regression / The Laplace Approximation</h3>
      	<span class="time">Wednesday, 2 October 2024</span><br />
      	<a href="files/lecture_notes/9.pdf">lecture notes</a>
      	<p>Additional Resources:</p>
      	<ul>
      	  <li>Book: Bishop PRML: Chapter 4 (Linear Models for Classificaiton).</li>
      	  <li>Book: Barber BRML: Section 18.2 (Classification).</li>
      	  <li>Book: Rasmussen and Williams GPML: Sections 3.1 and 3.2 (Classification Problems and Linear Models for Classification), <a href="http://www.gaussianprocess.org/gpml/chapters/RW3.pdf">available here</a>.</li>
      	  <li>Video: YouTube user mathematicalmonk has a <a href="https://www.youtube.com/watch?v=dtkGq9tdYcI">lecture about Bayesian logistic regression</a>.</li>
      	</ul>
      </section>
      <section>
      	<h3 class="lecture">Lecture 10: Gaussian Process Regression</h3>
      	<span class="time">Wednesday, 9 October 2024</span><br />
      	lecture notes &ndsh; see <a href="https://bayesoptbook.com/">Garnett BayesOpt</a>: Chapter 2</br>
	<a href="files/lecture_notes/10.pdf">lecture slides</a>
      	<p>Additional Resources/Notes:</p>
      	<ul>
      	  <li>Book: Rasmussen and Williams GPML: Sections 2.2 &ndash; 2.5, <a href="http://www.gaussianprocess.org/gpml/chapters/RW2.pdf">available here</a>.</li>
      	  <li>Book: Barber BRML: Chapter 19 (Gaussian processes).</li>
      	  <li>Video: Nando de Freitas has a lecture <a href="https://www.youtube.com/watch?v=4vGiHC35j9s">here.</a></li>
      	  <li>Video: Philipp Hennig has a series of lectures from the 2013 Machine Learning Summer School; <a href="http://www.youtube.com/watch?v=50Vgw11qn0o">part one is here</a>.  The slides, which have some cool animations, are <a href="http://mlss.tuebingen.mpg.de/2013/hennig_slides1.pdf">available here</a>.</li>
      	  <li>Video: Carl Rasmussen has a two-part introduction to Gaussian processes <a href="http://videolectures.net/mlss09uk_rasmussen_gp/">here</a>.</li>
      	  <li>Video: David MacKay gave an introduction to Gaussian processes <a href="http://videolectures.net/gpip06_mackay_gpb/">here</a>.</li>
      	</ul>
      </section>
      <section>
      	<h3 class="lecture">Lecture 11: Kernels</h3>
      	<span class="time">Monday, 14 October 2024</span><br />
      	lecture notes &ndash; see <a href="https://bayesoptbook.com/">Garnett BayesOpt</a>: Chapter 3</br>
      	<p>Resources/Notes:</p>
      	<ul>
      	  <li>Book: Bishop PRML: Chapter 6 (Kernel Methods).</li>
      	  <li>Book: Barber BRML: Section 19.3 (Covariance Functions).</li>
      	  <li>Book: Rasmussen and Williams GPML: Chapter 4 (Covariance Functions), <a href="http://www.gaussianprocess.org/gpml/chapters/RW4.pdf">available here</a>.</li>
      	  <li>Website: David Duvenaud has made a "kernel cookbook," <a href="https://www.cs.toronto.edu/~duvenaud/cookbook/">available here</a>.  This webpage also became <a href="https://raw.githubusercontent.com/duvenaud/phd-thesis/master/kernels.pdf">a chapter of his thesis.</a></li>
      	  <li>Website: Metacademy has a pages on <a href="http://metacademy.org/graphs/concepts/kernel_trick#lfocus=kernel_trick">the kernel trick</a> and <a href="http://metacademy.org/graphs/concepts/constructing_kernels#lfocus=constructing_kernels">constructing kernels</a>.</li>
      	</ul>
      </section>
      <section>
      	<h3 class="lecture">Lecture 12: A Quick Interlude</h3>
      	<span class="time">Wednesday, 16 October 2024</span><br />
      	<p>Additional Resources/Notes:</p>
      	<ul>
      	  <li>Slides: Roman Garnett (+ Simeon Bird, Shirley Ho, and Jeff Schneider), Finding Galaxies in the Shadows of Quasars, <a href="https://www.dropbox.com/s/hk5pkiwot0dy8k4/slides.pdf?dl=0">available here</a>.</li>
	  <li>Paper: Accompanying paper <a href="https://academic.oup.com/mnras/article/472/2/1850/4060725">available here.</a></li>
      	</ul>
      </section>
      <section>
      	<h3 class="lecture">Lecture 13: Bayesian Optimization</h3>
      	<span class="time">Monday, 21 October 2024</span><br />
      	<a href="https://docs.google.com/presentation/d/17zvrE8vzaaJvE96pHkTdm9wDfFUlrbMVyarGR4yP3Z4/edit?usp=sharing">lecture slides</a>
      	<p>Additional Resources/Notes:</p>
      	<ul>
	  <li>Book: Garnett, Bayesian Optimization, <a href="https://bayesoptbook.com">available here</a>.</li>
      	  <li>Paper: Niranjan Srinivas, Andreas Krause, Sham Kakade, and Mattias Seeger discuss the GP-UCB algorithm (including theoretical results!) <a href="http://las.ethz.ch/files/srinivas10gaussian.pdf">in this landmark paper</a>.</li>
      	  <li>Paper: Jasper Snoek, Hugo Larochelle, and Ryan P. Adams discuss the AutoML application of Bayesian optimization <a href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf">here</a>.</li>
      	</ul>
      </section>
      <section>
      	<h3 class="lecture">Lecture 14: Bayesian Quadrature</h3>
      	<span class="time">Wednesday, 23 October 2024</span><br />
      	<a href="files/lecture_notes/14.pdf">lecture notes</a>
      	<p>Additional Resources/Notes:</p>
      	<ul>
      	  <li>Slides: David Duvenaud has a set of slides introucing Bayesian quadrature, <a href="https://www.cs.toronto.edu/~duvenaud/talks/intro_bq.pdf">available here</a>.</li>
	  <li>Paper: Persi Diaconis's essay <a href="http://probabilistic-numerics.org/assets/pdf/Diaconis_1988.pdf">"Bayesian Numerical Analysis"</a> from 1998 provides an excellent introduction.</li>
      	  <li>Paper: Carl Rasmussen and Zoubin Ghahramani discuss Bayesian quadrature under the name "Bayesian Monte Carlo" <a href="http://mlg.eng.cam.ac.uk/zoubin/papers/RasGha03.pdf">in this paper</a>.  Many references therein are also interesting, especially the provocatively titled <a href="http://www.jstor.org/stable/2348519?seq=1#page_scan_tab_contents">Monte Carlo is fundamentally unsound</a> by Anthony O'Hagan.</li>
      	  <li>Paper: Tom Minka wrote a report on "Deriving quadrature rules from Gaussian processes," <a href="http://research.microsoft.com/en-us/um/people/minka/papers/quadrature.html">available here</a>.</li>
      	</ul>
      </section>
      <section>
      	<h3 class="lecture">Lecture 15: Sequential Decision Theory / Active Search</h3>
      	<span class="time">Wednesday, 6 November 2024</span><br />
      	<a href="https://docs.google.com/presentation/d/17zvrE8vzaaJvE96pHkTdm9wDfFUlrbMVyarGR4yP3Z4/edit?usp=sharing">lecture slides</a>
      	<p>Additional Resources/Notes:</p>
      	<ul>
      	  <li>Book: Berger, Sequential Decision Theory and Bayesian Analysis: Section 7.4 (Bayesian Sequential Analysis).</li>
	  <li>Book: DeGroot, Optimal Statistical Decisions: Part 4 (Sequential Decisions).</li>
	  <li>Book: Garnett, Bayesian Optimization: Section 11.11 (Non-Gaussian Observation Models and Active Search). </li>
	  <li>Paper: Garnett, et al., Bayesian Optimal Active Search and Surveying (ICML 2012).</li>
	  <li>Paper: Jiang, et al., Efficient Nonmyopic Active Search (ICML 2017).</li>
      	</ul>
      </section>
      <section>
      	<h3 class="lecture">Lecture 16: Importance Sampling, MCMC</h3>
      	<span class="time">Wednesday, 13 November 2024</span><br />
      	<a href="http://homepages.inf.ed.ac.uk/imurray2/teaching/09mlss/slides.pdf">lecture slides</a> (from Iain Murray's introduction at the 2009 Machine Learning Summer School)
      	<p>Additional Resources/Notes:</p>
      	<ul>
      	  <li>Video: You can watch Iain Murray present the slides himself <a href="https://www.youtube.com/watch?v=hn0Vfzcnmig">here</a>.</li>
      	  <li>Book: Barber BRML: Sections 27.4 (Markov Chain Monte Carlo (MCMC)), 27.3 (Gibbs Sampling), and 27.6 (Importance Sampling).</li>
      	  <li>Book: Bishop PRML: Sections 11.2 (Markov Chain Monte Carlo) and 11.3 (Gibbs Sampling).</li>
      	  <li>Videos: YouTube user mathematicalmonk has a chapter devoted to sampling methods (#17), beginning <a href="https://www.youtube.com/watch?v=AadKNJU1-lk&list=PLD0F06AA0D2E8FFBA&index=127">here</a>.</li>
      	</ul>
      </section>
      <section>
	<h3 class="lecture">Lecture 17: Practical Issues with GPs</h3>
	<span class="time">Monday, 18 November 2024</span><br />
      	lecture notes &ndash; see <a href="https://bayesoptbook.com/">Garnett BayesOpt</a>: Chapter 9</br>
      </section>
      <section>
      	<h3 class="lecture">Lecture 18: Bayesian additive regression trees (BART)</h3>
      	<span class="time">Wednesday, 20 November 2024</span><br />
      	<p>Additional Resources/Notes:</p>
      	<ul>
      	  <li>Paper: The original paper is <a href="https://arxiv.org/abs/0806.3286">here</a>.</li>
	</ul>
    </section>
      <!--
      <section>
      	<h3 class="lecture">Lecture 16: GP Classification / Assumed Density Filtering / Expectation Propagation</h3>
      	<span class="time">Wednesday, 30 October 2019</span><br />
      	<a href="files/lecture_notes/15.pdf">lecture notes</a>
      	<p>Additional Resources/Notes:</p>
      	<ul>
      	  <li>Book: Rasmussen and Williams GPML: Chapter 3 (Classification), especially Section 3.6 (Expecation Propagation) <a href="http://www.gaussianprocess.org/gpml/chapters/RW3.pdf">available here</a>.</li>
      	  <li>Book: Barber BRML: Section 28.8 (Expectation Propagation).</li>
      	  <li>Book: Bishop PRML: Section 10.7 (Expectation Propagation).</li>
      	  <li>Paper: <a href="http://arxiv.org/pdf/1412.4869v1.pdf">Expectation propagation as a way of life</a>.</li>
      	</ul>
      </section>
      <section>
	<h3 class="lecture">Lecture 18: Monte Carlo, Sampling, Rejection Sampling</h3>
	<span class="time">Wednesday, 6 November 2019</span><br />
	<a href="http://homepages.inf.ed.ac.uk/imurray2/teaching/09mlss/slides.pdf">lecture slides</a> (from Iain Murray's introduction at the 2009 Machine Learning Summer School)
	<ul>
	  <li>Book: Barber BRML: Section 27.1 (Sampling: Introduction).</li>
	  <li>Book: Bishop PRML: Section 11.1 (Basic Sampling Algorithms).</li>
	  <li>Videos: YouTube user mathematicalmonk has a chapter devoted to sampling methods (#17), beginning <a href="https://www.youtube.com/watch?v=AadKNJU1-lk&list=PLD0F06AA0D2E8FFBA&index=127">here</a>.</li>
	</ul>
    </section>
<!--
      <section>
      	<h3 class="lecture">Lecture 18: The Kalman Filter</h3>
      	<span class="time">Wednesday, 4 April 2018</span><br />
      	<a href="files/lecture_notes/14.pdf">lecture notes</a>
      	<p>Additional Resources/Notes:</p>
      	<ul>
      	  <li>Book: Bishop PRML: Section 13.3 (Linear Dynamical Systems).</li>
      	</ul>
      </section>
      <section>
      	<h3 class="lecture">Lecture 17: Bayesian Deep Learning?</h3>
      	<span class="time">Tuesday, 11 April 2017</span><br />
      	<p>Additional Resources/Notes:</p>
      	<ul>
      	  <li>Workshop: The <a href="http://bayesiandeeplearning.org">Bayesian Deep Learning</a> workshop at NIPS 2016.</li>
      	  <li>Video: A panel discussion entitled <a href="https://www.youtube.com/watch?v=HumFmLu3CJ8&t=1700s">"Is Bayesian deep learning the most brilliant thing ever?"</a> at the same workshop.</li>
	  <li>Blog: A great <a href="ttp://twiecki.github.io/blog/2016/06/01/bayesian-deep-learning/">blog post</a> about probabilistic programming and Bayesian deep learning.</li>
	  <li>Paper: A <a href="https://arxiv.org/pdf/1602.04133.pdf">nice paper on deep Gaussian processes.</a></li>
	  <li>Video: <a href="https://www.youtube.com/watch?v=YAb5C5_g-kk">"Modern Deep Learning through Bayesian Eyes"</a></li>
      	</ul>
      </section>
      -->
    </section>
    <section>
      <h2>Resources</h2>
      <section>
	<h3 class="resource">Books</h3>
	<p>There is no required book for this course.  That said, there are a wide variety of machine-learning books available, some of which are available for free online.  The following books all have a Bayesian slant to them:</p>
	<ul>
	  <li><em>Pattern Recognition and Machine Learning</em> (PRML) by <span class="author">Christopher M. Bishop.</span>  Covers many machine-learning topics thoroughly.  Definite Bayesian focus.  Can also be very mathematical and take some effort to read.</li>
	  <li><em>Bayesian Reasoning and Machine Learning</em> (BRML) by <span class="author">David Barber.</span>  Geared (as much as a machine-learning book can be!) towards computer scientists.  Lots of material on graphical models.  <a href="http://www.cs.ucl.ac.uk/staff/d.barber/brml/">Freely available online.</a></li>
	  <li><em>Gaussian Processes for Machine Learning</em> (GPML) by <span class="author">Carl Rasmussen and Christopher Williams.</span>  Excellent reference for Gaussian processes. <a href="http://gaussianprocess.org/gpml/">Freely available online.</a></li>
	  <li><em>Bayesian optimization</em> by <span class="author">me!</span> Although the focus is on Bayesian optimization, there is a lot of background material on Gaussian processes as well. <a href="https://bayesoptbook.com/">Freely available online.</a></li>
	  <li><em>Information Theory, Inference, and Learning Algorithms</em> by <span class="author">David J. C. Mackay.</span> Very strong focus on information theory.  If you have a background in physics or are interested in information theory, this is the book for you.  <a href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html">Freely available online.</a></li>
	</ul>
	For a more-frequentist perspective, check out the excellent <em>The Elements of Statistical Learning</em> by <span class="author">Trevor Hastie, Robert Tibshirani, and Jerome Friedman.</span>  <a href="https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf">Freely available online.</a>
	<section>
	  <h3 class="resource">Websites</h3>
	  <ul>
	    <li>I will post the source for lecture notes, demo code, etc. <a href="https://github.com/rmgarnett/cse515t">on this GitHub page.</a>  Even the source for the syllabus and this website are there.</li>
	    <li>I have created a Slack for this class (invite link at top of page).  Please post any questions about lectures, homework, etc here!  Chances are that someone else has the same question and we can all benefit from a public discussion.  If you have a question just for me and/or me and the TA, please also send a direct message via Slack rather than emailing us directly.</li>
	    <li>There are several relevant courses available on <a href="https://www.coursera.org/">Coursera.</a>  Coursera gives you access to video lecture series, often from world experts, all available for free!  In particular, the following three courses are all presented by leaders in the field:
	      <ul>
		<li><a href="https://www.coursera.org/course/ml">Andrew Ng's Machine Learning course</a> (Stanford University)</li>
		<li><a href="https://www.coursera.org/course/pgm">Daphne Koller's Probabilistic Graphical Models course</a> (Stanford University)</li>
	      </ul>
	    </li>
	  </ul>
	</section>
	<section>
	  <h3 class="resource">Other</h3>
	  <p>The <a href="http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf">Matrix Cookbook</a> by Kaare B. Petersen and Michael S. Pedersen can be incredibly useful for helping with tricky linear alegbra problems!
    </section>
  </body>
</html>
