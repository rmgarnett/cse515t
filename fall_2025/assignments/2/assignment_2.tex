\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[osf]{libertine}
\usepackage[scaled=0.8]{beramono}
\usepackage[margin=1.5in]{geometry}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{bm}

\usepackage{sectsty}
\sectionfont{\large}
\subsectionfont{\normalsize}

\usepackage{titlesec}
\titlespacing{\section}{0pt}{10pt plus 2pt minus 2pt}{0pt plus 2pt minus 0pt}
\titlespacing{\subsection}{0pt}{5pt plus 2pt minus 2pt}{0pt plus 2pt minus 0pt}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}

\newcommand{\acro}[1]{\textsc{\MakeLowercase{#1}}}
\newcommand{\given}{\mid}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\data}{\mc{D}}
\newcommand{\intd}[1]{\,\mathrm{d}{#1}}

\begin{document}

{\large \textbf{CSE 5015 (Fall 2025) Assignment 2}} \\
Due Wednesday, 8 October 2025 \\

\begin{enumerate}

\item

  We are going to consider a Bayesian hypothesis test that a coin is
  exactly fair. Let $\theta$ be the ``heads probability'' of a coin
  \[
    \Pr(\text{H}) = \theta.
  \]
  Let us place a somewhat atypical prior on $\theta$ that includes
  nontrivial prior probability that the coin is exactly fair (that is,
  $\theta = \nicefrac{1}{2}$):
  \[
    \Pr(\theta = \nicefrac{1}{2}) = 0.5
    \qquad
    p(\theta \given \theta \neq \nicefrac{1}{2}) = \mc{U}[0, 1].
  \]
  This can be interpreted as a prior that is a mixture between a uniform
  prior on the interval $[0, 1]$ with weight 0.5 (this is also a special
  of the beta distribution with $\alpha = \beta = 1$) and a ``point
  mass'' (Dirac delta function) at $\theta = \nicefrac{1}{2}$ with
  weight 0.5.

  Suppose we flip a coin $n = 10$ times and observe $x = 6$ ``heads.''
  What is the posterior distribution $p(\theta \given x, n)$?  What is
  the posterior probability that the coin is exactly fair?

  Note there is no probability density function corresponding to the
  prior. It will help to work in cases.

\item
  (Effect of weird priors.)  Let us consider the following set of
  observations. We flip a coin independently $n = 1\,000$ times and
  observe $x = 900$ successes. Call the unknown bias of the coin
  $\theta \in (0, 1)$.

  For each of the prior distributions $p(\theta)$ below, please:
  \begin{itemize}
  \item
    plot the prior distribution $p(\theta)$ over the range $0 < \theta < 1$
  \item
    plot the posterior distribution given the above data, $p(\theta
    \given \data)$, over the range $0 < \theta < 1$
  \item
    report the posterior mean,
    $\mathbb{E}[\theta \given \data] = \int \theta\,p(\theta\given\data)\intd\theta$.
    (Here you may want to look up the posterior mean of a beta distribution.)
  \end{itemize}

  (a) A uniform prior on $\theta$, which can be realized by selecting
  the beta distribution with $\alpha = \beta = 1$:
  \[
    p(\theta) = \mathcal{B}(\theta; \alpha = 1, \beta = 1).
  \]

  (b) A prior with extreme bias toward small values of $\theta$:
  \[
    p(\theta) = \mathcal{B}(\theta; \alpha = 1, \beta = 100).
  \]

  (c) A prior that has no support on values greater than $\theta =
  \nicefrac{1}{2}$:
  \[
    p(\theta) = \begin{cases} 2 & \theta <    \nicefrac{1}{2}; \\
                              0 & \theta \geq \nicefrac{1}{2}. \end{cases}
  \]

\item
  (Optimal \emph{Price is Right} bidding.)
  Suppose you have a standard normal belief about an unknown parameter
  $\theta$, $p(\theta) = \mc{N}(\theta; 0, 1^2)$.  You are asked to
  give a point estimate $\hat{\theta}$ of $\theta$, but are told that
  there is a heavy penalty for guessing too high.  The loss function is
  \begin{equation*}
    \ell(\hat{\theta}, \theta; c)
    =
    \begin{cases}
      (\theta - \hat{\theta})^2 & \hat{\theta}  <   \theta; \\
      c                         & \hat{\theta} \geq \theta
    \end{cases},
  \end{equation*}
  where $c > 0$ is a constant cost for overestimating.  What is the
  Bayesian estimator in this case?  How does it change as a function
  of $c$? Plot the optimal action as a function of $c$ for $0 < c <
  10$.

  \textbf{Hint:} you should minimize certain expressions you
  encounter numerically as an analytic solution may not be available / advisable.

\end{enumerate}

\end{document}
