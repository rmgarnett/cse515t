\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage[osf]{libertine}
\usepackage[scaled=0.8]{beramono}
\usepackage[margin=1.5in]{geometry}
\usepackage{url}
\usepackage{booktabs}
\usepackage{microtype}

\usepackage{sectsty}
\sectionfont{\large}
\subsectionfont{\normalsize}

\usepackage{titlesec}
\titlespacing{\section}{0pt}{10pt plus 2pt minus 2pt}{0pt plus 2pt minus 0pt}
\titlespacing{\subsection}{0pt}{5pt plus 2pt minus 2pt}{0pt plus 2pt minus 0pt}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}

\newcommand{\acro}[1]{\textsc{\MakeLowercase{#1}}}

\begin{document}

{\large \textbf{CSE 5015: Bayesian Methods in Machine Learning (Fall 2025)}} \\[1ex]

\begin{tabular}{rl}
               Instructor & Professor Roman Garnett                                 \\
                \acro{TA} & Ryan Zhang                                              \\
            Time/Location & Monday/Wednesday 10--11:20pm, Seigle 301                \\
   Office Hours (Garnett) & by appointment, always available on Slack               \\
 Office Hours (\acro{TA}) & \acro{TBA}                                              \\
               \acro{URL} & \url{https://www.cse.wustl.edu/~garnett/cse515t/fall_2025/}            \\
                   GitHub & \url{https://github.com/rmgarnett/cse515t}    \\
     Slack & \url{http://bit.ly/3UFNnfn}
\end{tabular}

\section*{Course Description}

This course will cover modern machine learning techniques from a Bayesian
probabilistic perspective. Bayesian probability allows us to model and reason
about all types of uncertainty. The result is a powerful, consistent framework
for approaching many problems that arise in machine learning, including
parameter estimation, model comparison, and decision making. We will begin with
a high-level introduction to Bayesian inference, then proceed to cover
more-advanced topics.

\textbf{This course is meant to lay the groundwork for research in these
areas.} If you are looking for a practical introduction with a focus on
implementation, etc. this may not be the best course for you.

\section*{Prerequisites}

We will make heavy use of mathematics in this course.  You should have a good
grasp of multivariable calculus (integration, partial derivation, maximization,
etc.), probability (conditional probability, expectations, etc.), and linear
algebra (solving linear systems, eigendecompositions, etc.).

Please note that this is not an introduction to machine learning; the \acro{CSE
  417T/517A} and \acro{ESE 417} courses fill that role.  I will assume prior
familiarity with the main concepts of machine learning: supervised and
unsupervised learning, classification, regression, clustering, etc.

\section*{Book}

There is no required book. For each lecture, I will provide a list of related
materials, including book chapters, videos, papers, code, etc.\ on the course
webpage.  These are to give you different viewpoints on the subject.  Hopefully
you can find one that suits you.

I have also posted links on the course webpage to several related books you may
find useful, many of which are available for free online.

\section*{Reading}

Regardless of what book(s) or other material(s) you may choose to consult,
\textbf{you are expected to spend some time outside of class engaging in reading and
  self-study.}

\section*{Assignments}

There will be a small number of assignments throughout the semester, with two
weeks available to complete each one.

The assignments will form 30\% of your grade, and each will have two types of
questions: traditional ``pencil-and-paper'' questions, and programming exercises
meant to give more insight into applying the techniques we will discuss on
actual data.  The former \emph{will not be corrected.}  If you make a reasonable
attempt to answer a question, I will give you full credit.  After each
assignment, I will provide solutions online.

The programming exercises will require you to implement some of the theoretical
ideas we discuss in class.  The point of these exercises is both to lead to a
better understanding by forcing a different viewpoint (that of the designer),
and also to enable interaction.  I encourage you to play with the data,
parameters, etc. associated with these exercises to see how the results change.
The point of the exercises is \emph{not} for me to judge your programming
skills, so \emph{please do not hand in your code.}  Rather, you should convey
your answers via plots, tables, and/or discussion, as appropriate.  As I don't
need to read your code, feel free to use any language you'd like.

\subsection*{Late policy}

Assignments will be due during class on the dates specified on the course
homepage.  I will allow you to turn in your assignment up to one class late with
no penalty.

\subsection*{Collaboration policy}

Please feel free to collaborate on the paper-and-pencil questions!  This is a
good way to gain a deeper understanding of the material.  Of course, you will be
expected to write up your answers separately.  Also feel free to collaborate on
a high level on the programming exercises, but please write your own code and
produce your own results.

\subsection*{AI policy}

If you would like to use Chat\acro{GPT} or a similar resource while completing
your assignments, that's fine with me. Just make sure you trust its output!

\section*{Midterm}

There will be an in-class midterm on a date to be determined later
(probably just before or just after fall break). This will count for 30\% of your
grade.

\section*{Project}

In the second half of the semester, you will complete a project, which will
comprise 40\% of your final grade.  You will have two possible paths to satisfy
this project requirement -- a free-form project or a supervised, guided project.

The main goal of the project is to give you hands-on experience applying
Bayesian methods to a real-world dataset.  The use of real-world data can have
many interesting (and potentially frustrating!) aspects that are difficult to
convey without getting your hands dirty.  The scope of the project is intended
to be more than a homework problem, but less than a full-fledged research paper.

We will talk more about the project a bit later in the semester.

\section*{Grading}

Your final grade will consist of the following weighted components:
\begin{center}
  \begin{tabular}{lc}
    \toprule
    component                    &   \% \\
    \midrule
    assignments                  & 30\% \\
    midterm                      & 30\% \\
    final project                & 40\% \\
    \bottomrule
  \end{tabular}
\end{center}

\section*{Topics}

An outline of the topics I expect to cover is below; this is subject to change,
more likely by deletion than addition.  If there is a particular topic you would
like me to spend more time on (or don't care about at all!), please let me know.

I will keep the course webpage updated with lecture-specific information and
resources.

\begin{itemize}
\item \textbf{Introduction to the Bayesian method:} review of probability,
  Bayes' theorem, Bayesian inference, Bayesian parameter estimation, Bayesian
  decision theory, Bayesian model selection.
\item \textbf{Approximate inference:} the Laplace approximation, variational
  Bayes, expectation propagation.
\item \textbf{Sampling methods:} rejection sampling, importance sampling, Markov
  chain Monte Carlo.
\item \textbf{Parametric models:} Bayesian linear regression, logistic
  regression, general linear models, basis expansions, mixture models, latent
  Dirichlet allocation.
\item \textbf{Nonparametric models:} Gaussian proesses for regression and
  classification.
\item \textbf{Bayesian numerical analysis:} Bayesian optimization, Bayesian
  quadrature.
\end{itemize}

\section*{Other resources}

The University has also put together a list of important policies and resources
that I recommend you review if you haven't already.\footnote{\url{https://provost.wustl.edu/syllabi-resources-and-template-language-danforth-campus/}}


\end{document}
